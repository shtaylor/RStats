---
title: "Week 7 - Homework"
author: "STAT 420, Summer 2018, Shane Taylor"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
library(readr)
```

## Exercise 1 (EPA Emissions Data)

For this exercise, we will use the data stored in [`epa2015.csv`](epa2015.csv). It contains detailed descriptions of 4,411 vehicles manufactured in 2015 that were used for fuel economy testing [as performed by the Environment Protection Agency]( https://www3.epa.gov/otaq/tcldata.htm). The variables in the dataset are:  

- `Make` - Manufacturer
- `Model` - Model of vehicle
- `ID` - Manufacturer defined vehicle identification number within EPA's computer system (not a VIN number)
- `disp` - Cubic inch displacement of test vehicle
- `type` - Car, truck, or both (for vehicles that meet specifications of both car and truck, like smaller SUVs or crossovers)
- `horse` - Rated horsepower, in foot-pounds per second
- `cyl` - Number of cylinders
- `lockup` - Vehicle has transmission lockup; N or Y
- `drive` - Drivetrain system code
    - A = All-wheel drive
    - F = Front-wheel drive
    - P = Part-time 4-wheel drive
    - R = Rear-wheel drive
    - 4 = 4-wheel drive
- `weight` - Test weight, in pounds
- `axleratio` - Axle ratio
- `nvratio` - n/v ratio (engine speed versus vehicle speed at 50 mph)
- `THC` - Total hydrocarbons, in grams per mile (g/mi)
- `CO` - Carbon monoxide (a regulated pollutant), in g/mi
- `CO2` - Carbon dioxide (the primary byproduct of all fossil fuel combustion), in g/mi
- `mpg` - Fuel economy, in miles per gallon

We will attempt to model `CO2` using both `horse` and `type`. In practice, we would use many more predictors, but limiting ourselves to these two, one numeric and one factor, will allow us to create a number of plots.

Load the data, and check its structure using `str()`. Verify that `type` is a factor; if not, coerce it to be a factor.


**(a)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.
- Fit a simple linear regression model with `CO2` as the response and only `horse` as the predictor.
- Add the fitted regression line to the scatterplot. Comment on how well this line models the data.
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. (Interestingly, the dataset gives the wrong drivetrain for most Subarus in this dataset, as they are almost all listed as `F`, when they are in fact all-wheel drive.)

***

##### **Solution 1a**

First we load the data and ensure that `epa2015$type` is registered as a factor variable.

```{r, message=FALSE}
epa2015 = read_csv("epa2015.csv")
epa2015$type = as.factor(epa2015$type)
```

Next we create the simple scatter plot.

```{r}
plot(CO2 ~ horse, data = epa2015, 
     col = as.numeric(type) + 1, main = "CO2 vs horsepower", 
     xlab = "horsepower", ylab = "CO2 emmissions" )
legend("topleft", levels(epa2015$type), col = 1:length(levels(epa2015$type))+1, pch = 1)

```

Then, using a simple linear model we add the regression line.

```{r}
CO2.slm = lm(CO2 ~ horse, data = epa2015)

plot(CO2 ~ horse, data = epa2015, 
     col = as.numeric(type) + 1, main = "CO2 vs horsepower", 
     xlab = "horsepower", ylab = "CO2 emmissions" )
abline(coef(CO2.slm)[1], coef(CO2.slm)[2], lty = 1, lwd = 2)
legend("topleft", levels(epa2015$type), col = 1:length(levels(epa2015$type))+1, pch = 1)
```

The above plot seems to fit the data to an extent (especially the car data), but we can see that there is a clear pattern present within the "truck" and "both" category that is not being reflected within the line.

Since this is a simple linear model that doesn't consider the factor variables, the estimate for the change in CO2 for a one foot-pound per second increase in `horse` for a vehicle of type `car` is just the slope of the line, which is `r coef(CO2.slm)[2]`.

To get a 90% prediction interval we use:

```{r}
subaru_impreza_wagon = data.frame("horse" = 148, "type" = factor("Both", levels = levels(epa2015$type)))
CO2.slm.pred_int = predict(CO2.slm, newdata = subaru_impreza_wagon, interval = "prediction", level = .9)
```

This tells us that our prediction interval is $\big( `r CO2.slm.pred_int[1,2]`, `r CO2.slm.pred_int[1,3]`\big)$

***

**(b)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.
- Fit an additive multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.
- Add the fitted regression "lines" to the scatterplot with the same colors as their respective points (one line for each vehicle type). Comment on how well this line models the data. 
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. 

***

##### **Solution 1b**

First we create a simple scatter plot.

```{r}
plot_colors = c("Darkorange", "Darkgrey", "Dodgerblue")
plot(CO2 ~ horse, data = epa2015, 
     col = plot_colors[type], main = "CO2 vs horsepower (by engine type)", 
     xlab = "horsepower", ylab = "CO2 emmissions", pch = as.numeric(type) )
legend("topleft", levels(epa2015$type), col = plot_colors, pch = 1:3)

```

Next we make an additive regression model with `CO2` as the response and `horse` and `type` as the predictors.

```{r}
CO2.additivelm = lm(CO2 ~ horse + type, data = epa2015)
```

Next we add the regression lines to the plot. Notice how we add the applicable intercept coefficients to shift the lines up and down.

```{r}

int_both = coef(CO2.additivelm)[1]
int_car = int_both + coef(CO2.additivelm)[3]
int_truck = int_both + coef(CO2.additivelm)[4]
slope_all_vehicles = coef(CO2.additivelm)[2]

plot(CO2 ~ horse, data = epa2015, 
     col = plot_colors[type], main = "CO2 vs horsepower (by engine type)", 
     xlab = "horsepower", ylab = "CO2 emmissions", pch = as.numeric(type) )
abline(int_both, slope_all_vehicles, lty = 1, lwd = 2, col = plot_colors[1])
abline(int_car, slope_all_vehicles, lty = 2, lwd = 2, col = plot_colors[2])
abline(int_truck, slope_all_vehicles, lty = 3, lwd = 2, col = plot_colors[3])
legend("topleft", levels(epa2015$type), col = plot_colors, lty = 1:3, pch = 1:3)
```

Since this is just an additive model, the slope for all the lines are the same. So, the average change in CO2 for a one foot-pound per second increase in horse for a vehicle of type car is just the slope of the linear model, which is `r slope_all_vehicles`.

Since this is an additive model, our prediction will at least consider the type of vehicle the Subaru Impreza is. The prediction interval is given by:

```{r}
CO2.additivelm.pred_int = predict(CO2.additivelm, newdata = subaru_impreza_wagon, interval = "prediction", level = .9)
```

This tells us that our prediction interval is $\big( `r CO2.additivelm.pred_int[1,2]`, `r CO2.additivelm.pred_int[1,3]`\big)$

***

**(c)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`. 
- Fit an interaction multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.
- Add the fitted regression "lines" to the scatterplot with the same colors as their respective points (one line for each vehicle type). Comment on how well this line models the data. 
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. 

***

##### **Solution 1c**

First we make the scatter plot.

```{r}
plot_colors = c("Darkorange", "Darkgrey", "Dodgerblue")
plot(CO2 ~ horse, data = epa2015, 
     col = plot_colors[type], main = "CO2 vs horsepower (by engine type)", 
     xlab = "horsepower", ylab = "CO2 emmissions", pch = as.numeric(type) )
legend("topleft", levels(epa2015$type), col = plot_colors, pch = 1:3)

```

Next we make the interaction model.

```{r}
CO2.interactionlm = lm(CO2 ~ horse * type, data = epa2015)
```

Then, we add the regression lines for the three different types. Notice how we add the applicable coefficients to shift the intercept and adjust the slopes.

```{r}
int_both_2 = coef(CO2.interactionlm)[1]
int_car_2 = int_both_2 + coef(CO2.interactionlm)[3]
int_truck_2 = int_both_2 + coef(CO2.interactionlm)[4]

slope_both = coef(CO2.interactionlm)[2]
slope_car = slope_both + coef(CO2.interactionlm)[5]
slope_truck = slope_both + coef(CO2.interactionlm)[6]

plot(CO2 ~ horse, data = epa2015, 
     col = plot_colors[type], main = "CO2 vs horsepower (by engine type)", 
     xlab = "horsepower", ylab = "CO2 emmissions", pch = as.numeric(type) )
abline(int_both_2, slope_both, lty = 1, lwd = 2, col = plot_colors[1])
abline(int_car_2, slope_car, lty = 2, lwd = 2, col = plot_colors[2])
abline(int_truck_2, slope_truck, lty = 3, lwd = 2, col = plot_colors[3])
legend("topleft", levels(epa2015$type), col = plot_colors, lty = 1:3, pch = 1:3)
```

With the interaction model we finally have different slopes for different types of vehicles. So, the average change in CO2 for a one foot-pound per second increase in horse for a vehicle of type car is given by `slope_car`, which is `r slope_car`.

Since this is an interaction model, our prediction will consider the type of vehicle the Subaru Impreza is and use that slope to make its determination. The prediction interval is given by:

```{r}
CO2.interactionlm.pred_int = predict(CO2.interactionlm, newdata = subaru_impreza_wagon, interval = "prediction", level = .9)
```

This tells us that our prediction interval is $\big( `r CO2.interactionlm.pred_int[1,2]`, `r CO2.interactionlm.pred_int[1,3]`\big)$.

***

**(d)** Based on the previous plots, you probably already have an opinion on the best model. Now use an ANOVA $F$-test to compare the additive and interaction models. Based on this test and a significance level of $\alpha = 0.10$, which model is preferred?

***

##### **Solution 1d**

```{r}
add_int.anova = anova(CO2.additivelm, CO2.interactionlm)
add_int.anova
```


With a p-value of `r add_int.anova[[6]][2]`, which is less than $\alpha = 0.10$, we reject the null hypothesis and determine that the differences introduced by the interaction model are significant. This makes the interaction model our preferred model.

***


## Exercise 2 (Hospital SUPPORT Data, White Blood Cells)

For this exercise, we will use the data stored in [`hospital.csv`](hospital.csv). It contains a random sample of 580 seriously ill hospitalized patients from a famous study called "SUPPORT" (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment). As the name suggests, the purpose of the study was to determine what factors affected or predicted outcomes, such as how long a patient remained in the hospital. The variables in the dataset are:  
 
- `Days` - Days to death or hospital discharge
- `Age` - Age on day of hospital admission
- `Sex` - Female or male
- `Comorbidity` - Patient diagnosed with more than one chronic disease
- `EdYears` - Years of education
- `Education` - Education level; high or low
- `Income` - Income level; high or low
- `Charges` - Hospital charges, in dollars
- `Care` - Level of care required; high or low
- `Race` - Non-white or white
- `Pressure` - Blood pressure, in mmHg
- `Blood` - White blood cell count, in gm/dL
- `Rate` - Heart rate, in bpm

For this exercise, we will use `Age`, `Education`, `Income`, and `Sex` in an attempt to model `Blood`. Essentially, we are attempting to model white blood cell count using only demographic information.

**(a)** Load the data, and check its structure using `str()`. Verify that `Education`, `Income`, and `Sex` are factors; if not, coerce them to be factors. What are the levels of `Education`, `Income`, and `Sex`?

***

##### **Solution 2a**

First we load the data from the `.csv` file and check its structure using `str()`.

```{r, message=FALSE}
hospital = read_csv("hospital.csv")
str(hospital)
```

Then we check to see if the applicable variables are factor variables.

```{r}
is.factor(hospital$Education)
is.factor(hospital$Income)
is.factor(hospital$Sex)
```

Since they are not, we make them factor variables.

```{r}
hospital$Education = as.factor(hospital$Education)
hospital$Income = as.factor(hospital$Income)
hospital$Sex = as.factor(hospital$Sex)
```

The following displays the factor levels for the applicable variables. First up is `Education`.

```{r}
levels(hospital$Education)
```

Then we have `Income`.

```{r}
levels(hospital$Income)

```

Then we have `Sex`.

```{r}
levels(hospital$Sex)
```

***

**(b)** Fit an additive multiple regression model with `Blood` as the response using `Age`, `Education`, `Income`, and `Sex` as predictors. What does `R` choose as the reference level for `Education`, `Income`, and `Sex`?

***

##### **Solution 2b**

First we create an additive linear model with `Blood` as the response using `Age`, `Education`, `Income`, and `Sex` as predictors.

```{r}
hospital.add_lm = lm(Blood ~ Age + Education + Income + Sex, data = hospital)
coef(hospital.add_lm)
```

Since the coefficient names contain `Educationlow`, `Incomelow` and `Sexmale`, then the reference level is the suffix not listed. That means the reference levels for for `Education`, `Income`, and `Sex` are `high`, `high`, and `female` respectively.

***

**(c)** Fit a multiple regression model with `Blood` as the response. Use the main effects of `Age`, `Education`, `Income`, and `Sex`, as well as the interaction of `Sex` with `Age` and the interaction of `Sex` and `Income`. Use a statistical test to compare this model to the additive model using a significance level of $\alpha = 0.10$. Which do you prefer?

***

##### **Solution 2c**

First we create the linear model with the applicable interactions. Then, we take a look at the analysis of variance table.

```{r}
hospital.mlm = lm(Blood ~ Age + Education + Income + Sex + Sex:Age + Sex:Income, data = hospital)
anova(hospital.add_lm, hospital.mlm)
```

With a p-value of `r anova(hospital.add_lm, hospital.mlm)[[6]][2]`, it doesn't quite meet our minimum significance level we set of $\alpha = 0.10$. Therefore, we'd prefer the more simple model in this case (that is, the additive model). 

***

**(d)** Fit a model similar to that in **(c)**, but additionally add the interaction between `Income` and `Age` as well as a three-way interaction between `Age`, `Income`, and `Sex`. Use a statistical test to compare this model to the preferred model from **(c)** using a significance level of $\alpha = 0.10$. Which do you prefer?

***

##### **Solution 2d**

We fit a different interaction model as specified. Then we take a look at the analysis of variance table.

```{r}
hospital.mlm_alt = lm(Blood ~ Age + Education + Income + Sex + Sex:Age + Sex:Income + Income:Age + Age:Income:Sex, data = hospital)
anova(hospital.add_lm, hospital.mlm_alt)
```

With a p-value of `r anova(hospital.add_lm, hospital.mlm_alt)[[6]][2]` we reject the null hypothesis and conclude that the differences introduced by this new interaction model are significant. This makes the new interaction model our preferred model.

***

**(e)** Using the model in **(d)**, give an estimate of the change in average `Blood` for a one-unit increase in `Age` for a highly educated, low income, male patient.


***

##### **Solution 2e**

We first look at the coefficients of the model

```{r}
coef(hospital.mlm_alt)
```

We essentially just need to add up the coefficients for `Age`, `Age:Sexmale`, `Age:Incomelow` and `Age:Incomelow:Sexmale`. Therefore, the estimate of the change in average `Blood` for a one-unit increase in `Age` for a highly educated, low income, male patient is `r sum(coef(hospital.mlm_alt)[c("Age", "Age:Sexmale", "Age:Incomelow", "Age:Incomelow:Sexmale")])`.

***


## Exercise 3 (Hospital SUPPORT Data, Stay Duration)

For this exercise, we will again use the data stored in [`hospital.csv`](hospital.csv). It contains a random sample of 580 seriously ill hospitalized patients from a famous study called "SUPPORT" (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment). As the name suggests, the purpose of the study was to determine what factors affected or predicted outcomes, such as how long a patient remained in the hospital. The variables in the dataset are:  
 
- `Days` - Days to death or hospital discharge
- `Age` - Age on day of hospital admission
- `Sex` - Female or male
- `Comorbidity` - Patient diagnosed with more than one chronic disease
- `EdYears` - Years of education
- `Education` - Education level; high or low
- `Income` - Income level; high or low
- `Charges` - Hospital charges, in dollars
- `Care` - Level of care required; high or low
- `Race` - Non-white or white
- `Pressure` - Blood pressure, in mmHg
- `Blood` - White blood cell count, in gm/dL
- `Rate` - Heart rate, in bpm

For this exercise, we will use `Blood`, `Pressure`, and `Rate` in an attempt to model `Days`. Essentially, we are attempting to model the time spent in the hospital using only health metrics measured at the hospital.

Consider the model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon,
\]

where

- $Y$ is `Days`
- $x_1$ is `Blood`
- $x_2$ is `Pressure`
- $x_3$ is `Rate`.

**(a)** Fit the model above. Also fit a smaller model using the provided `R` code.

```{r, eval = FALSE}
days_add = lm(Days ~ Pressure + Blood + Rate, data = hospital)
```

Use a statistical test to compare the two models. Report the following:

- The null and alternative hypotheses in terms of the model given in the exercise description
- The value of the test statistic
- The p-value of the test
- A statistical decision using a significance level of $\alpha = 0.10$
- Which model you prefer

***

##### **Solution 3a**

We fit the interaction and additive model, and then take a look at the analysis of variance table.

```{r}
days_int = lm(Days ~ Blood * Pressure * Rate, data = hospital)
days_add = lm(Days ~ Pressure + Blood + Rate, data = hospital)
days.anova = anova(days_add, days_int)
```

The null hypothesis is that interactions between Blood:Pressure, Blood:Rate, Pressure:Rate, Blood:Pressure:Rate are insignificant. Mathematically, we could say that $H_0 : \beta_4 = \beta_5 = \beta_6 = \beta_7 = 0$.

The alternative hypothesis is that at least one of the relationships listed above is non-zero. Mathematically we could say that $H_1 : \exists \ \beta_i \ne 0$ for some $i \in \{4,5,6,7\}$.

Since the additive model is a nested model, we used `anova()` and got a test statistic of `r days.anova[["F"]][2]`.

Since we got a p-value of `r days.anova[["Pr(>F)"]][2]`, we reject the null hypothesis.

Since we rejected the null hypothesis we know there is significance within the interactions of the variables, so we prefer the interaction model.

***

**(b)** Give an expression based on the model in the exercise description for the true change in length of hospital stay in days for a 1 bpm increase in `Rate` for a patient with a `Pressure` of 139 mmHg and a `Blood` of 10 gm/dL. Your answer should be a linear function of the $\beta$s.

***

##### **Solution 3b**

We want to consider the coefficient in front of $x_3$. To accomplish this we combine all the terms involving $x_3$ and factor out $x_3$. First, let's look at the displayed coefficients. 

```{r}
coef(days_int)
```

Going by the definitions of $x_1, x_2,$ and $x_3$ listed above, where

- $Y$ is `Days`
- $x_1$ is `Blood`
- $x_2$ is `Pressure`
- $x_3$ is `Rate`

then coefficient of $x_3$ is:

$$
\beta_3 + \beta_5x_1 + \beta_6x_2 + \beta_7x_1x_2
$$
Using our known values we get:

$$
-0.16089311 + (0.00711664 \cdot 10)\  + \ (0.00368262 \cdot 139)\ + \ (-0.00009251 \cdot 10 \cdot 139)
$$
We can make a function that lets you plug in your values for `Pressure` and `Blood`:

```{r}

get_rate_coef = function(pressure, blood){
  as.numeric(coef(days_int)["Rate"] + coef(days_int)["Blood:Rate"]*blood + coef(days_int)["Pressure:Rate"]*pressure + coef(days_int)["Blood:Pressure:Rate"]*pressure*blood)
}

get_rate_coef(pressure = 139, blood = 10)
```


***

**(c)** Give an expression based on the additive model in part **(a)** for the true change in length of hospital stay in days for a 1 bpm increase in `Rate` for a patient with a `Pressure` of 139 mmHg and a `Blood` of 10 gm/dL. Your answer should be a linear function of the $\beta$s.


***

##### **Solution 3c**

In a similar vein to the previous problem, we want to look at the coefficient in front of $x_3$. The additive model is given by:

$$
Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3
$$

where

- $Y$ is `Days`
- $x_1$ is `Blood`
- $x_2$ is `Pressure`
- $x_3$ is `Rate`.

So in this case, the true change in length of hospital stay in days for a 1 bpm increase in `Rate` for a patient with a `Pressure` of 139 mmHg and a `Blood` of 10 gm/dL is just $\beta_3$, or programatically it is `coef(days_add)["Rate"]` = `r coef(days_add)["Rate"]`.

***


## Exercise 4 ($t$-test Is a Linear Model)

In this exercise, we will try to convince ourselves that a two-sample $t$-test assuming equal variance is the same as a $t$-test for the coefficient in front of a single two-level factor variable (dummy variable) in a linear model.

First, we set up the data frame that we will use throughout.

```{r}
n = 30

sim_data = data.frame(
  groups = c(rep("A", n / 2), rep("B", n / 2)),
  values = rep(0, n))
str(sim_data)
```

We will use a total sample size of `30`, `15` for each group. The `groups` variable splits the data into two groups, `A` and `B`, which will be the grouping variable for the $t$-test and a factor variable in a regression. The `values` variable will store simulated data.

We will repeat the following process a number of times.

```{r}
set.seed(420)
sim_data$values = rnorm(n, mean = 42, sd = 3.5) # simulate response data
summary(lm(values ~ groups, data = sim_data))
t.test(values ~ groups, data = sim_data, var.equal = TRUE)
```

We use `lm()` to test

\[
H_0: \beta_1 = 0
\]

for the model

\[
Y = \beta_0 + \beta_1 x_1 + \epsilon
\]

where $Y$ is the values of interest, and $x_1$ is a dummy variable that splits the data in two. We will let `R` take care of the dummy variable.

We use `t.test()` to test

\[
H_0: \mu_A = \mu_B
\]

where $\mu_A$ is the mean for the `A` group, and $\mu_B$ is the mean for the `B` group.

The following code sets up some variables for storage.

```{r}
num_sims = 300
lm_t = rep(0, num_sims)
lm_p = rep(0, num_sims)
tt_t = rep(0, num_sims)
tt_p = rep(0, num_sims)
```

- `lm_t` will store the test statistic for the test $H_0: \beta_1 = 0$.
- `lm_p` will store the p-value for the test $H_0: \beta_1 = 0$.
- `tt_t` will store the test statistic for the test $H_0: \mu_A = \mu_B$.
- `tt_p` will store the p-value for the test $H_0: \mu_A = \mu_B$.

The variable `num_sims` controls how many times we will repeat this process, which we have chosen to be `300`.

**(a)** Set a seed equal to your birthday. Then write code that repeats the above process `300` times. Each time, store the appropriate values in `lm_t`, `lm_p`, `tt_t`, and `tt_p`. Specifically, each time you should use `sim_data$values = rnorm(n, mean = 42, sd = 3.5)` to update the data. The grouping will always stay the same.

***

##### **Solution 4a**

We perform our simulations and set our values.

```{r}
set.seed(01171992)
for (i in 1:300) {
  sim_data$values = rnorm(n, mean = 42, sd = 3.5)
  sim_test = t.test(values ~ groups, data = sim_data, var.equal = TRUE)
  sim_summary = summary(lm(values ~ groups, data = sim_data))
  lm_t[i] = sim_summary$coefficients[2,3]
  lm_p[i] = sim_summary$coefficients[2,4]
  tt_t[i] = sim_test$statistic
  tt_p[i] = sim_test$p.value
}

```


***

**(b)** Report the value obtained by running `mean(lm_t == tt_t)`, which tells us what proportion of the test statistics is equal. The result may be extremely surprising!

***

##### **Solution 4b**

```{r}
mean(lm_t == tt_t)
```

This tells us that none of the entries are pair-wise exactly equal according to the machine's level of precision. But looking at the vectors in the `R` viewer, I can see that there is indeed something going on that isn't being represented with this measure.

***

**(c)** Report the value obtained by running `mean(lm_p == tt_p)`, which tells us what proportion of the p-values is equal. The result may be extremely surprising!

***

##### **Solution 4c**

```{r}
mean(lm_p == tt_p)
```

This tells us that very few of the entries are pair-wise exactly equal according to the machine's level of precision. But looking at the vectors in the `R` viewer, I can see that there is indeed something going on that isn't being represented with this measure.

***

**(d)** If you have done everything correctly so far, your answers to the last two parts won't indicate the equivalence we want to show! What the heck is going on here? The first issue is one of using a computer to do calculations. When a computer checks for equality, it demands **equality**; nothing can be different. However, when a computer performs calculations, it can only do so with a certain level of precision. So, if we calculate two quantities we know to be analytically equal, they can differ numerically. Instead of `mean(lm_p == tt_p)` run `all.equal(lm_p, tt_p)`. This will perform a similar calculation, but with a very small error tolerance for each equality. What is the result of running this code? What does it mean?

***

##### **Solution 4d**

```{r}
all.equal(lm_p, tt_p)
```

This means that all the p-values for the t-test of the significance of $\beta_1$ are equal to the p-values for the means of the two groups.

***

**(e)** Your answer in **(d)** should now make much more sense. Then what is going on with the test statistics? Look at the values stored in `lm_t` and `tt_t`. What do you notice? Is there a relationship between the two? Can you explain why this is happening?

***

##### **Solution 4e**

I noticed that `lm_t` = `-1 * tt_t`, as seen below:

```{r}
all.equal(lm_t, -tt_t)
```

Since multiplication is a linear operation, then there exists a linear transformation between the two statistics. We've previously shown that the mean is linear, that is: 

$$
\hat{\mu}(X) + \hat{\mu}(Y) = \hat{\mu}(X + Y)
$$

and, for some constant $c$,

$$
\hat{\mu}(c \cdot X) = c\cdot\hat{\mu}(X)
$$

Since we can perform a linear transformation on `tt_t` to get the same result from `lm_t`, then the t-test is also linear, and thus a linear model. My best guess for why the two statistics have opposite values is that `R` may choose different reference factor for the calculations (that is, it chooses group `A` for one reference factor, and chooses group `B` for the other).

***


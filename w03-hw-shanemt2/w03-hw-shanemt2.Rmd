---
title: "Week 3 - Homework"
author: "STAT 420, Summer 2018, Shane Taylor"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---


***

## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

***

##### Solution 1a

```{r}
library(MASS)
cats = MASS::cats
catmodel = lm(Hwt ~ Bwt, data = cats)
catmodel.betahat0 = catmodel$coefficients[[1]]
catmodel.betahat1 = catmodel$coefficients[[2]]

catmodel.SE = summary(catmodel)$coefficients[2,2]



catmodel.pval = 2 * pt(-abs(catmodel.t), df = nrow(cats) - 2)
```


To summarize, we get that:


- $\hat{\beta_0}$ = `r catmodel.betahat0`
- $\hat{\beta_1}$ = `r catmodel.betahat1`
- $H_0$ : $\beta_1 = 0$
- $H_1$ : $\beta_1 \ne 0$
- $t$ = `r catmodel.t`
- p-value = `r catmodel.pval`
- Since `r catmodel.pval` $< \alpha = 0.05$ then we would reject $H_0$.
- Since we've rejected $H_0$ then it provides evidence that there is a linear relationshp between the body weight and the heart weight of a cat.


***

**(b)** Calculate a 90% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.

***

##### Solution 1b

We use the `confint()` function:

```{r}
beta_1_confint = confint(catmodel, parm = "Bwt", level = 0.9)
```


This confidence interval tells us that we are 90% confident that $\beta_1 \in$ (`r beta_1_confint[1,]`). This means that we are 90% confident that on average when we see a 1 kg increase in a cat's body weight that we will see a cat's heart weight increase between `r beta_1_confint[1,1]` and `r beta_1_confint[1,2]` grams.


***

**(c)** Calculate a 99% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

***

##### Solution 1c

```{r}
beta_0_confint = confint(catmodel, parm = "(Intercept)", level = 0.99)
```

This confidence interval tells us that we are 99% confident that $\beta_0 \in$ (`r beta_0_confint[1,]`). This means that we are 99% confident that as a cat's body weight approaches 0, its heart weight will approach a value between `r beta_0_confint[1,1]` and `r beta_0_confint[1,2]` grams.

***

**(d)** Use a 99% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?

***

##### Solution 1d

This is a confidence interval for a mean response. We use the `predict()` function and some special function parameters.

```{r}
catmodel.MR = predict(catmodel, newdata = data.frame(Bwt = c(2.1, 2.8)), interval = c("confidence"), level = 0.99)
catmodel.MR
```

To summarize the table above, the 99% confidence interval for the mean heart weight of a cat with a body weight of 2.1 kg is (`r catmodel.MR[1,2]`, `r catmodel.MR[1,3]`) grams and the 99% confidence interval for the mean heart weight of a cat with a body weight of 2.8 kg is (`r catmodel.MR[2,2]`, `r catmodel.MR[2,3]`) grams. As to why the first interval is wider, we must first take a look at what the mean body weight of a cat is.

```{r}
mean(cats$Bwt)
```

Since 2.1 is further from `r mean(cats$Bwt)` than 2.8 is, then the confidence interval for the mean reponse at 2.1 is wider than when it's at 2.8.

***

**(e)** Use a 99% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.

***

##### Solution 1e

We will now use the optional parameter `interval = c("prediction")` to determine this interval.

```{r}
catmodel.PI = predict(catmodel, newdata = data.frame(Bwt = c(2.8, 4.2)), interval = c("prediction"), level = 0.99)
catmodel.PI
```

Again, to summarize the table above, the 99% prediction interval for the heart weight of a cat with a body weight of 2.8 kg is (`r catmodel.PI[1,2]`, `r catmodel.PI[1,3]`) grams and the 99% prediction interval for the heart weight of a cat with a body weight of 4.2 kg is (`r catmodel.PI[2,2]`, `r catmodel.PI[2,3]`) grams.

***

**(f)** Create a scatterplot of the data. Add the regression line, 90% confidence bands, and 90% prediction bands.

***

##### Solution 1f

We need to make a bunch of predictions for the data set in order to provide enough data to the `lines()` function so that it can render the lines accurately.

```{r}

bwt_grid = seq(min(cats$Bwt), max(cats$Bwt), by = 0.01)

catmodel.MR_band = predict(catmodel, newdata = data.frame(Bwt = bwt_grid), interval = c("confidence"), level = 0.9)

catmodel.PI_band = predict(catmodel, newdata = data.frame(Bwt = bwt_grid), interval = c("prediction"), level = 0.9)

```

Now we make the plot:

```{r}
plot(cats$Bwt, cats$Hwt,
     xlab = "Body weight (kilograms)",
     ylab = "Heart weight (grams)",
     main = "Body weight vs Heart weight in cats",
     pch = 20,
     cex = 2,
     col = "darkorange")
abline(catmodel, lwd = 3, col = "darkred")
lines(bwt_grid, catmodel.PI_band[, "lwr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(bwt_grid, catmodel.PI_band[, "upr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(bwt_grid, catmodel.MR_band[, "lwr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(bwt_grid, catmodel.MR_band[, "upr"], col = "dodgerblue", lwd = 3, lty = 2)
```



***

**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$

Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

***

##### Solution 1g

Our critical value is given by:

```{r}
catmodel.t = (catmodel.betahat1 - 4)/catmodel.SE
```



\[
t = \frac{\hat{\beta_1} - \beta_{10}}{SE[\hat{\beta_1}]} \approx \frac{`r catmodel.betahat1` - 4}{`r catmodel.SE`} = `r catmodel.t`
\]

Our p-value is given by:

```{r}
catmodel.p = 2* pt(q = abs(catmodel.t), df = length(cats$Bwt) - 2, lower.tail = FALSE)
catmodel.p
```

Since `r catmodel.p` > $\alpha = 0.05$, then we fail to reject the null hypothesis.

***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

***

##### Solution 2a

First let's fit our model to the data:

```{r}
ozone_wind_model = lm(ozone ~ wind, data = Ozone)
```

Our null and alternative hypotheses are as follows:
  
- $H_0 : \beta_{1} = 0$ 
- $H_1 : \beta_{1} \ne 0$

By default, `summary(ozone_wind_model)` reports the t and p value for the null and alternative hypotheses listed above:

```{r}
ozone_wind_model.t = summary(ozone_wind_model)$coefficients[2,3]
ozone_wind_model.p = summary(ozone_wind_model)$coefficients[2,4]
```

- $t = `r ozone_wind_model.t`$
- $p = `r ozone_wind_model.p`$

Since `r ozone_wind_model.p` $> \alpha = 0.01$ then we fail to reject $H_0$. In the context of the problem, this indicates that there is little evidence to suggest a linear relationship between wind and ozone measurements.

***

**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

***

##### Solution 2b

Again, let's fit our model to the data:

```{r}
ozone_temp_model = lm(ozone ~ temp, data = Ozone)
```

Our null and alternative hypotheses are as follows:
  
- $H_0 : \beta_{1} = 0$ 
- $H_1 : \beta_{1} \ne 0$


By default, `summary(ozone_wind_model)` reports the t and p value for the null and alternative hypotheses listed above:

```{r}
ozone_temp_model.t = summary(ozone_temp_model)$coefficients[2,3]
ozone_temp_model.p = summary(ozone_temp_model)$coefficients[2,4]
```

- $t = `r ozone_temp_model.t`$
- $p = `r ozone_temp_model.p`$

Since `r ozone_temp_model.p` $< \alpha = 0.01$ then we reject $H_0$. In the context of the problem, this indicates that there is evidence to suggest that a linear relationship exists between temperature and ozone measurements.


***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19920117
set.seed(birthday)
n = 50
x = seq(0, 10, length = n)
```

***

##### Solution 3a



```{r}

simulate = function(num_sims = 1, x, beta1, beta0, variance){
  betahat1 = rep(NA, num_sims)
  betahat0 = rep(NA, num_sims)
  stdev = sqrt(variance)
  for (i in 1:num_sims) {
    reg = lm(beta1 * x + beta0 + rnorm(length(x), 0, stdev) ~ x)
    betahat0[i] = reg$coefficients[[1]]
    betahat1[i] = reg$coefficients[[2]]
  }
  data.frame(beta_hat_0 = betahat0, beta_hat_1 = betahat1)
}

coeffs = simulate(num_sims = 2000, x = x, beta1 = 3.25, beta0 = -5, variance = 16)
mean_x = mean(x)
Sxx = sum((x-mean_x)^2)
betahat0_sd = sqrt(16*((1/50) + (mean_x^2/Sxx)))
betahat1_sd = sqrt(16/Sxx)
```


***

**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values

***

##### Solution 3b

| | $\hat{\beta}_0$ | $\hat{\beta}_1$ |
|-|-----------------|-----------------|
|True values| -5 | 3.25 |
|Mean of simulated values | `r mean(coeffs$beta_hat_0)` | `r mean(coeffs$beta_hat_1)` |
|True standard deviation | $\sqrt{16\big(\frac{1}{50} + \frac{`r mean_x`^2}{`r Sxx`}\big)} \approx `r betahat0_sd`$ | $\sqrt{\frac{16}{`r Sxx`}} \approx `r betahat1_sd`$|
|Simulated standard deviation | `r sd(coeffs$beta_hat_0)` | `r sd(coeffs$beta_hat_1)` |

***

**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.
- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.


***

##### Solution 3c

```{r}
par(mfrow = c(1,2))
hist(coeffs$beta_hat_0, prob = TRUE, breaks =20, 
     xlab = expression(hat(beta)[0]), main = "", border = "dodgerblue")
curve(dnorm(x, mean = -5, sd = betahat0_sd),
      col = "darkorange", add = TRUE, lwd = 3)
hist(coeffs$beta_hat_1, prob = TRUE, breaks = 20, 
     xlab = expression(hat(beta)[1]), main = "", border = "dodgerblue")
curve(dnorm(x, mean = 3.25, sd = betahat1_sd),
      col = "darkorange", add = TRUE, lwd = 3)
```

***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19920117
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)
```

***

##### Solution 4a

We'll begin by modifying the `simulate()` function from before:

```{r}
simulate2 = function(num_sims = 1, x, beta1, beta0, variance){
  betahat1 = rep(NA, num_sims)
  Se = rep(NA, num_sims)
  stdev = sqrt(variance)
  for (i in 1:num_sims) {
    yhat = beta1 * x + beta0 + rnorm(length(x), 0, stdev)
    regr = lm(yhat ~ x)
    Se[i] = summary(regr)$sigma
    betahat1[i] = regr$coefficients[[2]]
  }
  data.frame(beta_hat_1 = betahat1, S_e = Se)
}

simuls = simulate2(2500, x = x, beta1 = 2, beta0 = 5, variance = 9)
```


***

**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

***

##### Solution 4b

The confidence interval is 
$$\hat{\beta}_1 \pm t_{\alpha/2, n - 2} \cdot \frac{s_e}{\sqrt{S_{xx}}}$$ 
where,

$$P(t_{n-2} > t_{\alpha/2, n - 2}) = \alpha/2$$

With a confidence interval of .95, then $\alpha/2 = 0.025$. This means that $t_{\alpha/2, n - 2}$ is given by:

```{r}
t = qt(.025, 23, lower.tail = FALSE)
t
```

Now we need to calculate $S_{xx}$:

```{r}
Sxx = sum((x-mean(x))^2)
Sxx
```

Now we have all the pieces we need:

```{r}
upper_95 = simuls$beta_hat_1 + (t/Sxx) * simuls$S_e
lower_95 = simuls$beta_hat_1 - (t/Sxx) * simuls$S_e
```

***

**(c)** What proportion of these intervals contains the true value of $\beta_1$?

***

##### Solution 4c

We can use `&` on two logical vectors to receive a logical vector as the output. Then, we can run `mean()` on this resulting vector and it will treat each `TRUE`s as a 1, and give the proportion.

```{r}
prop = mean((2 >= lower_95 ) & (2 <= upper_95))
prop
```

The proportion of intervals that contain the true value of $\beta_1$ is only `r prop`.

***

**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?

***

##### Solution 4d

For this question we are simply look at the proportion of the intervals that don't contain 0.

```{r}
prop = mean(0 <= lower_95 | 0 >= upper_95)
prop
```

The proportion of simulations that would reject the test is `r prop`. 

***

**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

***

##### Solution 4e

This is essentially identical to 4b. We begin by calculating the necessary pieces of the puzzle. $\alpha/2$ is 0.005 now.

```{r}
t = qt(.005, 23, lower.tail = FALSE)
t

# Sxx and S_e are still the same.
# The only thing that has changed is t, thus making the dimensions larger.

upper_99 = simuls$beta_hat_1 + (t/Sxx) * simuls$S_e
lower_99 = simuls$beta_hat_1 - (t/Sxx) * simuls$S_e
```


***

**(f)** What proportion of these intervals contains the true value of $\beta_1$?

***

##### Solution 4f

Note: $\beta_1 = 2$

```{r}
prop = mean((2 >= lower_99 ) & (2 <= upper_99))
prop
```


***

**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?


***

##### Solution 4g

The intervals that don't contain 0 are the ones that would reject $H_0$.

```{r}
prop = mean((0 <= lower_99) | (0 >= upper_99))
prop
```

The proportion of our randomly generated samples at a 99% confidence level would be `r prop`. 


***


## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$


(a) Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
- This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval

***

##### Solution 5a

Per a hint from Piazza, the only thing different between a 99% and 95% confidence interval is the t-value used in the multiplication. Dividing by the default 95% confidence t and multiplying by your choice of t will resize the interval accordingly.

```{r}
calc_pred_int = function(model, newdata, level = 0.95){
  
  alpha = 1 - level
  a = alpha/2
  
  t95 = qt(p = .025, df = model$df.residual, lower.tail = FALSE)
  t = qt(p = a, df = model$df.residual, lower.tail = FALSE)
  
  prediction = predict(model, newdata = newdata, interval = c("confidence"))
  
  offset = prediction[3] - prediction[1]
  
  offset = ifelse(0.95 %in% level, offset, offset * t / t95)
  
  predint = c(prediction[1], prediction[1] - offset, prediction[1] + offset)
  names(predint) = c("estimate", "lower", "upper")
  predint
}

```

***

(b) After writing the function, run this code:

```{r, eval = FALSE}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(cat_model, newcat_1)
```

***

##### Solution 5b

```{r}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(catmodel, newcat_1)
```


***

**(c)** After writing the function, run this code:

```{r, eval = FALSE}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(cat_model, newcat_2, level = 0.99)
```

***

##### Solution 5c

```{r}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(catmodel, newcat_2, level = 0.99)
```





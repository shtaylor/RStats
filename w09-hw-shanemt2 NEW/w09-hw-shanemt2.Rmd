---
title: "Week 9 - Homework"
author: "STAT 420, Summer 2018, Shane Taylor"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---



***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

```{r, message=FALSE}
library(knitr)
library(kableExtra)
```


## Exercise 1 (`longley` Macroeconomic Data)

The built-in dataset `longley` contains macroeconomic data for predicting employment. We will attempt to model the `Employed` variable.

```{r, eval = FALSE}
View(longley)
?longley
```

**(a)** What is the largest correlation between any pair of predictors in the dataset?

***

##### **Solution 1a**

Before we jump straight into the problem, let's take a look at the relationships between the variables using the `pairs()` function:

```{r}
pairs(longley)
```

From the diagram above, we can see that `Year` and `Population` seem to have a high correlation, which makes sense considering the population typically grows  year after year. Now, let's figure this out a little more programmatically. To do so we use the `cor()` function, and set the diagnonal values to 0 since, a variable is obviously highly correlated with itself. Then, we call `which(..., arr.ind = TRUE)` to return the array index of the max value.

```{r}
longley.cor = cor(longley)
for (i in 1:nrow(longley.cor)) {
  longley.cor[i,i] = 0
}
longley.cor
which(longley.cor == max(longley.cor), arr.ind = TRUE)
```

How surprising! I was certain that `Year` and `Population` were the most highly correlated from the `pairs()` plot, but according to the output of the function and the table above, it seems the correlation of `GNP` and `Year` is just a bit higher than `Year` and `Population`. 

***

**(b)** Fit a model with `Employed` as the response and the remaining variables as predictors. Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

***

##### **Solution 1b**

To get access to the function `vif()` we must access it from the `faraway` package.

```{r}
library(faraway)
```

We fit the model and calculate the variance inflation factors.

```{r}
employed.lm_1 = lm(Employed ~ ., data = longley)
vif(employed.lm_1)
```

According to the heuristic that any VIF exceeding **5** indicates potential collinearity, it seems that `GNP.deflator`, `GNP`, `Unemployed`, `Population`, and `Year` all reek of multicollinearity issues. We can see that `GNP` has the highest VIF with a VIF of `r round(as.numeric(max(vif(employed.lm_1))), 2)`

***

**(c)** What proportion of the observed variation in `Population` is explained by a linear relationship with the other predictors?

***

##### **Solution 1c**

To calculate the proportion of the variation of `Population` that is explained by the other predictors, we create a linear model with `Population` as the response, and all other variables (except `Employed`) as the predictors. Then we calculate the `r.squared` value of the model.

```{r}
population.lm = lm(Population ~ . - Employed, data = longley)
summary(population.lm)$r.squared
```

This tells us that about `r summary(population.lm)$r.squared*100`% of the observed variation in `Population` is explained by the other predictors.

***

**(d)** Calculate the partial correlation coefficient for `Population` and `Employed` **with the effects of the other predictors removed**.

***

##### **Solution 1d**

To do this we will have to create another model with all predictors except `Population`. Then, we will look at the correlation between the residuals of the two models.

```{r}

employed_no_population.lm = lm(Employed ~ . - Population, data = longley)

cor(resid(employed_no_population.lm), resid(population.lm))

```

Since this value is close to zero, then it means that the variation of `Employed` that is unexplained by all the other predictors (besides `Population`) shows very little correlation with the variation of `Population` that is not explained by the other predictors.

***

**(e)** Fit a new model with `Employed` as the response and the predictors from the model in **(b)** that were significant. (Use $\alpha = 0.05$.) Calculate and report the variance inflation factor for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

***

##### **Solution 1e**

First, we select all of the variable names with a low enough p-value. Then, we fit a linear model using them. We leverage the fact that we can use a string as the argument for `lm()` to programmatically select the significant predictors.

```{r}
# We use 2:nrow(summary(employed.lm_1)$coefficients) below because we don't want to include (Intercept)
signif_variables = names(which(summary(employed.lm_1)$coefficients[2:nrow(summary(employed.lm_1)$coefficients),"Pr(>|t|)"] < 0.05))
employed.lm_2 = lm(paste("Employed", " ~ ", paste(signif_variables, collapse = " + ")), data = longley)
vif(employed.lm_2)
```

The largest VIF is ``r names(which.max(vif(employed.lm_2)))`` with a VIF of `r round(as.numeric(max(vif(employed.lm_2))), 3)`. Since all of these are less than **5**, we conclude that none of these suggest multicollinearity.

***

**(f)** Use an $F$-test to compare the models in parts **(b)** and **(e)**. Report the following:

- The null hypothesis
- The test statistic
- The distribution of the test statistic under the null hypothesis
- The p-value
- A decision
- Which model you prefer, **(b)** or **(e)**

***

##### **Solution 1f**

```{r}
employed.anova = anova(employed.lm_2, employed.lm_1)
employed.anova

e2.s = summary(employed.lm_2)
e2.s$fstatistic
```

- $H_0$ : All predictors in `employed.lm_1` that are not in `employed.lm_2` are equal to zero.
- The test statistic is `r employed.anova$F[2]`
- The distribution of the test statistic under the null hypothesis is `F(num_df = 3, den_df = 12)`
- The p-value is `r employed.anova[2, "Pr(>F)"]`
- We fail to reject the null hypothesis and say that none of the additional predictors added by the larger model are significant
- The preferred model is `Employed ~ Unemployed + Armed.Forces + Year`

***

**(g)** Check the assumptions of the model chosen in part **(f)**. Do any assumptions appear to be violated?

```{r, echo = FALSE}
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}
```


***

##### **Solution 1g**

```{r}
par(mfrow = c(1,2))
plot_fitted_resid(employed.lm_2)
plot_qq(employed.lm_2)

```

It's difficult to say with confidence since the data set is so small, but it does appear to me that the data is mostly normally distributed, but the points at the extreme ends of the Q-Q plot to appear to deviate quite a bit from the line. There may be an issue with the constant variance assumption, but I'm not 100%. The two most extreme points are far to the right, indicating that the variance may be a function of the values of the predictor variables.

***


## Exercise 2 (`Credit` Data)

For this exercise, use the `Credit` data from the `ISLR` package. Use the following code to remove the `ID` variable which is not useful for modeling.

```{r}
library(ISLR)
data(Credit)
Credit = subset(Credit, select = -c(ID))
```

Use `?Credit` to learn about this dataset.

**(a)** Find a "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `135`
- Obtain an adjusted $R^2$ above `0.90`
- Fail to reject the Breusch-Pagan test with an $\alpha$ of $0.01$
- Use fewer than 10 $\beta$ parameters

Store your model in a variable called `mod_a`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r, eval = FALSE}
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```

***

```{r}
library(leaps)
```


##### **Solution 2a**

I was unable to complete the procedure without removing the outliers from the data. Forum posts on Piazza seemed to indicate that this was OK per the Office hours with the TA's. I start with a very large model, and use that to find the initial model that I will use to determine the outliers using the Cook's Distance heuristic.


```{r fig.width=10}
#pairs(Credit)
all_balance_model = summary(regsubsets(Balance ~ (. - Student)^2 + I(Income ^ 2) + I(Limit ^ 2) + I(Rating ^ 2) + I(Cards ^ 2) + I(Age ^ 2) + I(Education ^ 2), data = Credit, really.big = TRUE))
```

Now, I check the adjusted $R^2$ values

```{r}
all_balance_model$adjr2

which(all_balance_model$which[6,])
```

I use the model with 7 parameters (including the intercept) to gauge the Cook's Distance to determine outliers. I do this so that with the different dummy variables of the factor variables, I stay under 10 parameters.

```{r}
mod_a = lm(Balance ~ Rating + I(Income^2) + I(Limit ^ 2) + Income:Limit + Cards:Gender + Age:Gender, data = Credit)

```

Next, I check the Cook's Distance. I use a more extreme heuristic ($\frac{8}{n}$) so as to not remove too many outliers.

```{r}

influential_indices = as.vector(which(cooks.distance(mod_a) > 8/length(cooks.distance(mod_a))))
influential_indices
```

We see that there are a few influential indices that are definititely outliers.

```{r}
NewCredit = Credit[-influential_indices,]
mod_a = lm(Balance ~ Rating + I(Income^2) + I(Limit ^ 2) + Income:Limit + Cards:Gender + Age:Gender, data = NewCredit)

get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```

We can see that all of the requirements are met using this model.

***

**(b)** Find another "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `125`
- Obtain an adjusted $R^2$ above `0.91`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters

Store your model in a variable called `mod_b`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r, eval = FALSE}
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```


***

##### **Solution 2b**

We create a model using a different approach this time using the `step()` function. This model was much easier to find than the previous and did not require removing any outliers.

```{r}

mod_b_start = lm(Balance ~ .^2, data= Credit)

mod_b = step(mod_b_start, direction = "backward", trace = 0, k = 2)
```

We test the data using the supplied test functions

```{r}
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```

As we can see, this model fits all of the necessary requirements.

***


## Exercise 3 (`Sacramento` Housing Data)

For this exercise, use the `Sacramento` data from the `caret` package. Use the following code to perform some preprocessing of the data.

```{r, message=FALSE}
library(caret)
library(ggplot2)
data(Sacramento)
sac_data = Sacramento
sac_data$limits = factor(ifelse(sac_data$city == "SACRAMENTO", "in", "out"))
sac_data = subset(sac_data, select = -c(city, zip))
```

Instead of using the `city` or `zip` variables that exist in the dataset, we will simply create a variable (`limits`) indicating whether or not a house is technically within the city limits of Sacramento. (We do this because they would both be factor variables with a **large** number of levels. This is a choice that is made due to laziness, not necessarily because it is justified. Think about what issues these variables might cause.)

Use `?Sacramento` to learn more about this dataset.

A plot of longitude versus latitude gives us a sense of where the city limits are.

```{r}
qplot(y = longitude, x = latitude, data = sac_data,
      col = limits, main = "Sacramento City Limits ")
```

After these modifications, we test-train split the data.

```{r}
set.seed(420)
sac_trn_idx  = sample(nrow(sac_data), size = trunc(0.80 * nrow(sac_data)))
sac_trn_data = sac_data[sac_trn_idx, ]
sac_tst_data = sac_data[-sac_trn_idx, ]
```

The training data should be used for all model fitting. Our goal is to find a model that is useful for predicting home prices.

**(a)** Find a "good" model for `price`. Use any methods seen in class. The model should reach a LOOCV-RMSE below 77,500 in the training data. Do not use any transformations of the response variable.

***

##### **Solution 3a**

```{r}
pairs(sac_trn_data)
```


We begin using a backward search starting with all two-way interactions and the square root of square feet.

```{r}
str(sac_trn_data)
```


```{r}
price_mod_start = lm(price ~ .^2 + I(sqft ^ .5), data = sac_trn_data)
price_mod = step(price_mod_start, direction = "backward", trace = 0)

get_loocv_rmse(price_mod)
```

We have found a model with a small enough LOOCV-RMSE.

Below are the coefficients of the model.

```{r}
coef(price_mod)
```


***

**(b)** Is a model that achieves a LOOCV-RMSE below 77,500 useful in this case? That is, is an average error of 77,500 low enough when predicting home prices? To further investigate, use the held-out test data and your model from part **(a)** to do two things:

- Calculate the average percent error:
\[
\frac{1}{n}\sum_i\frac{|\text{predicted}_i - \text{actual}_i|}{\text{predicted}_i} \times 100
\]
- Plot the predicted versus the actual values and add the line $y = x$.

Based on all of this information, argue whether or not this model is useful.


***

##### **Solution 3b**

We calculate the absolute value of the residuals (which is just the deviation from the actual value), divide by the actual value to get the percent of the error, and then take the mean.

```{r}

price_predict = predict(price_mod, newdata = sac_tst_data)
n = nrow(sac_tst_data)
avg_percent_error = (1/n) * sum(abs(price_predict - sac_tst_data$price)/sac_tst_data$price)
avg_percent_error
```

We get an average percent error of `r avg_percent_error * 100` %, which is relatively large. But let's look at the data visually and try to see how well it fits.

```{r}
plot(price_predict, sac_tst_data$price, col = "dodgerblue", pch = 20, 
     main = "Predicted Vs Actual Prices", ylab = "Predicted Price", xlab = "Actual Price")
abline(a = 0, b = 1, col = "darkorange", lwd = 2)
```

Overall the data does match the prediction, but there are definitely lot of discrepancies. Even with such a large percent error, it seems as though these models still help identify overall trends.

***


## Exercise 4 (Does It Work?)

In this exercise, we will investigate how well backwards AIC and BIC actually perform. For either to be "working" correctly, they should result in a low number of both **false positives** and **false negatives**. In model selection,

- **False Positive**, FP: Incorrectly including a variable in the model. Including a *non-significant* variable
- **False Negative**, FN: Incorrectly excluding a variable in the model. Excluding a *significant* variable

Consider the **true** model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_7 x_7 + \beta_8 x_8 + \beta_9 x_9 + \beta_{10} x_{10} + \epsilon
\]

where $\epsilon \sim N(0, \sigma^2 = 4)$. The true values of the $\beta$ parameters are given in the `R` code below.

```{r}
beta_0  = 1
beta_1  = -1
beta_2  = 2
beta_3  = -2
beta_4  = 1
beta_5  = 1
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
```

Then, as we have specified them, some variables are significant, and some are not. We store their names in `R` variables for use later.

```{r}
not_sig  = c("x_6", "x_7", "x_8", "x_9", "x_10")
signif = c("x_1", "x_2", "x_3", "x_4", "x_5")
```

We now simulate values for these `x` variables, which we will use throughout part **(a)**.

```{r}
set.seed(420)
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
```

We then combine these into a data frame and simulate `y` according to the true model.

```{r}
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

We do a quick check to make sure everything looks correct.

```{r}
head(sim_data_1)
```

Now, we fit an incorrect model.

```{r}
fit = lm(y ~ x_1 + x_2 + x_6 + x_7, data = sim_data_1)
coef(fit)
```

Notice, we have coefficients for `x_1`, `x_2`, `x_6`, and `x_7`. This means that `x_6` and `x_7` are false positives, while `x_3`, `x_4`, and `x_5` are false negatives.

To detect the false negatives, use:

```{r}
# which are false negatives?
!(signif %in% names(coef(fit)))
```

To detect the false positives, use:

```{r}
# which are false positives?
names(coef(fit)) %in% not_sig
```

Note that in both cases, you could `sum()` the result to obtain the number of false negatives or positives.

**(a)** Set a seed equal to your birthday; then, using the given data for each `x` variable above in `sim_data_1`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table.

***

##### **Solution 4a**

```{r}
set.seed(01171992)
num_false_positives_aic = rep(0, 300)
num_false_negatives_aic = rep(0,300)

num_false_positives_bic = rep(0, 300)
num_false_negatives_bic = rep(0,300)


# n = 100 from above
for (i in 1:300) {
  sim_data = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
    y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
        beta_5 * x_5 + rnorm(n, 0 , sigma)
  )
  sim_lm_start = lm(y ~ ., data = sim_data)
  sim_lm_aic = step(sim_lm_start, direction = "backward", trace = 0)
  sim_lm_bic = step(sim_lm_start, direction = "backward", trace = 0, k = log(n))
  num_false_negatives_aic[i] = sum(!(signif %in% names(coef(sim_lm_aic))))
  num_false_positives_aic[i] = sum(names(coef(sim_lm_aic)) %in% not_sig)
  num_false_negatives_bic[i] = sum(!(signif %in% names(coef(sim_lm_bic))))
  num_false_positives_bic[i] = sum(names(coef(sim_lm_bic)) %in% not_sig)
  
  
}



```

```{r}
results = data.frame("Average Rate" = rep(0,4), "Total" = rep(0,4), "Max" = rep(0,4), "Min" = rep(0,4))
rownames(results) = c("Num False Negatives (AIC)", "Num False Negatives (BIC)", "Num False Positives (AIC)", "Num False Positives (BIC)")
results[1, ] = c(mean(num_false_negatives_aic), sum(num_false_negatives_aic), max(num_false_negatives_aic), min(num_false_negatives_aic))
results[2, ] = c(mean(num_false_negatives_bic), sum(num_false_negatives_bic), max(num_false_negatives_bic), min(num_false_negatives_bic))
results[3, ] = c(mean(num_false_positives_aic), sum(num_false_positives_aic), max(num_false_positives_aic), min(num_false_positives_aic))
results[4, ] = c(mean(num_false_positives_bic), sum(num_false_positives_bic), max(num_false_positives_bic), min(num_false_positives_bic))
kable(results) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

It appears that both of these models are good at preventing against False Negatives since in the simulation it always correctly included all of the significant variables. However, it seems both AIC and BIC model selection resulted in many False Positives, with AIC having a much higher rate of incidence.

***

**(b)** Set a seed equal to your birthday; then, using the given data for each `x` variable below in `sim_data_2`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table. Also compare to your answers in part **(a)** and suggest a reason for any differences.

```{r}
set.seed(420)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)

sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```


***

##### **Solution 4b**

```{r}
set.seed(01171992)
num_false_positives_aic2 = rep(0, 300)
num_false_negatives_aic2 = rep(0,300)

num_false_positives_bic2 = rep(0, 300)
num_false_negatives_bic2 = rep(0,300)


# n = 100 from above
for (i in 1:300) {
  sim_data = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
  )
  sim_lm_start = lm(y ~ ., data = sim_data)
  sim_lm_aic = step(sim_lm_start, direction = "backward", trace = 0)
  sim_lm_bic = step(sim_lm_start, direction = "backward", trace = 0, k = log(n))
  num_false_negatives_aic2[i] = sum(!(signif %in% names(coef(sim_lm_aic))))
  num_false_positives_aic2[i] = sum(names(coef(sim_lm_aic)) %in% not_sig)
  num_false_negatives_bic2[i] = sum(!(signif %in% names(coef(sim_lm_bic))))
  num_false_positives_bic2[i] = sum(names(coef(sim_lm_bic)) %in% not_sig)
  
  
}
```

```{r}
results2 = data.frame("Average Rate" = rep(0,4), "Total" = rep(0,4), "Max" = rep(0,4), "Min" = rep(0,4))
rownames(results) = c("Num False Negatives (AIC)", "Num False Negatives (BIC)", "Num False Positives (AIC)", "Num False Positives (BIC)")
results2[1, ] = c(mean(num_false_negatives_aic2), sum(num_false_negatives_aic2), max(num_false_negatives_aic2), min(num_false_negatives_aic2))
results2[2, ] = c(mean(num_false_negatives_bic2), sum(num_false_negatives_bic2), max(num_false_negatives_bic2), min(num_false_negatives_bic2))
results2[3, ] = c(mean(num_false_positives_aic2), sum(num_false_positives_aic2), max(num_false_positives_aic2), min(num_false_positives_aic2))
results2[4, ] = c(mean(num_false_positives_bic2), sum(num_false_positives_bic2), max(num_false_positives_bic2), min(num_false_positives_bic2))
kable(results2) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

Now that `x_8, x_9,` and `x_10` are defined in terms of other variables, it seems that both models have begun making False Negative errors. The reason for this is that the models have a difficult time choosing between `x_8` and `x_1` since `x_8` is defined in terms of `x_1` and so somtimes it will choose `x_8` over `x_1`. Additionally, both methods have increased their number of False Positives errors. The reason for this is the same, since it's more likely that the two processes include the `x_8, x_9,` and `x_10` since they're defined in terms of other variables.

***

### END